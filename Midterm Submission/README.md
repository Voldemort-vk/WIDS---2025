Week 1:
In the first week, we were taught about different methods to encode an image. Neural networks came to be very handy for this work. We were taught about the details of neural networks, forward prop., back prop., feature extraction and many more. we were also taught OpenCV for image processing. In the first assignment, we also build an artificail neural networks from scratch to classify the hand written digits in the MNIST dataset. Also we were taught about the CNNs and how they use convolution to extract features from an image and how its better than basic NN in image processing.

Week 2:
In the second week we were taught abbout Natural Language Processing. We were taught many different tools in NLP like tokenization, lemmetization, regular expressions and others. We learnt how to handle big textual documents. Then we learnt basic RNNs and there use in text prediction or translation to different laguage. Then came the LSTM and finally the most important part the Transformers. We learnt about transformers, there structure, how they generate contextual features of a text using KQV matrices. We used Word2Vec and learnt about it. Also we learnt how to do pipelining using Hugging Face.

Week 3:
In this week we are learning how to combine all that we have learned till now into a single model. We want to encode an image using CNNs or ViTs and then decode them in text form using transformers we learnt and generate a text related to the image.
